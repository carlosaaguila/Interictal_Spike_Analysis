{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58439215-75ff-4382-9a63-c459a68a7f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.io import loadmat, savemat\n",
    "import mat73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf724a-7a17-44d3-b118-7e4b290bd4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_1 = mat73.loadmat('/Users/carlosaguila/Downloads/split_1.mat')\n",
    "fs = 512;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f802f-ce72-4a50-b0a7-3a9d26fe8683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_max(sequence_split, values):\n",
    "    seq_0_concat = np.concatenate(sequence_split)\n",
    "    seq_0_concat = np.concatenate(seq_0_concat) #double concatenate so that everything is in a array corresponding - basically turns into GDF\n",
    "    ch_uniq = np.unique(seq_0_concat[:,0]) #finds the unique channels in each run_time\n",
    "    #finds the mean of the max of each channel's spike index.\n",
    "    meanmax_per_ch = [];\n",
    "    all_max = []\n",
    "    for ch in ch_uniq:\n",
    "        x = np.where(seq_0_concat[:,0] == ch)[0] #index where all spikes are per channel per run_time\n",
    "        max_in_ch = []\n",
    "        chs = []\n",
    "        for i in x:\n",
    "            val = values[seq_0_concat[i,1]-20:seq_0_concat[i,1]+20, ch] #finding value at the spike and channel from x\n",
    "            val_max = np.max(np.abs(val))\n",
    "            max_in_ch.append(val_max)\n",
    "            chs.append(np.unique(ch))\n",
    "        meanmax_per_ch.append(np.mean(max_in_ch))\n",
    "        all_max.append([np.concatenate(chs),max_in_ch])\n",
    "        \n",
    "    chs2 = (np.concatenate(np.transpose(all_max)[0]))\n",
    "    maxs2 = (np.concatenate(np.transpose(all_max)[1]))\n",
    "    all_max_2 = [chs2,maxs2] #reshape of maxs in all channels.\n",
    "    return meanmax_per_ch, ch_uniq, all_max_2\n",
    "\n",
    "def ALL_mean_max(split_1): #input would be the complete matrix assuming 'values_all' and 'seqs_all' are the base names\n",
    "    mean_max_ALL = []\n",
    "    ch_uniq_ALL = []\n",
    "    max_I_ALL = []\n",
    "    \n",
    "    for I in range(len(split_1['values_all'][0])):\n",
    "        values_gdf_I = split_1['values_all'][0,I]\n",
    "        seq_I = split_1['seqs_all'][0,I]\n",
    "        mean_max_seq_I, ch_uniq_I, all_max_I = mean_max(seq_I,values_gdf_I)\n",
    "        mean_max_ALL.append(mean_max_seq_I)\n",
    "        ch_uniq_ALL.append(ch_uniq_I)\n",
    "        max_I_ALL.append(all_max_I)\n",
    "    \n",
    "    ch_uniq_AL_C = np.concatenate(ch_uniq_ALL)\n",
    "    mean_max_ALL_C = np.concatenate(mean_max_ALL)\n",
    "    ALL_CH = []\n",
    "    ALL_maxvalues = []\n",
    "    for s in range(len(max_I_ALL)):\n",
    "        ALL_CH.append((max_I_ALL[s][0]))\n",
    "        ALL_maxvalues.append((max_I_ALL[s][1]))\n",
    "    ALL_CH = np.concatenate(ALL_CH)\n",
    "    ALL_maxvalues = np.concatenate(ALL_maxvalues)\n",
    "    max_I_ALL = [ALL_CH,ALL_maxvalues]\n",
    "    ALL_mean_max_with_ch = [ch_uniq_AL_C,mean_max_ALL_C]\n",
    "    return ALL_mean_max_with_ch, max_I_ALL\n",
    "\n",
    "#creates a compilation of every channel in all gdfs and there respective means of max absolute peak values \n",
    "def meanofmeanmax_per_ch(split_1): #input is the complete split file.\n",
    "    all_mean_max, max_I_all = ALL_mean_max(split_1) #uses ALL_mean_max function to get you a complete list of concatenated mean max's\n",
    "\n",
    "    #code to get means of means per channel.\n",
    "    all_mean_max = np.transpose(all_mean_max)\n",
    "    max_I_all = np.transpose(max_I_all)\n",
    "    popmean= np.nanmean(max_I_all[:,1])\n",
    "    popstd = np.nanstd(max_I_all[:,1])\n",
    "    ch_uniq = np.unique(all_mean_max[:,0]) #finds the unique channels in concatenated list\n",
    "    ch_uniq_ALL = np.unique(max_I_all[:,0])\n",
    "    \n",
    "    means = []\n",
    "    means_from_all_maxes = []\n",
    "    std_from_all_maxes = []\n",
    "    for ch in ch_uniq:\n",
    "        x = np.where(all_mean_max[:,0] == ch)[0]#index where all spikes are per channel per run_time\n",
    "        means.append(np.mean(all_mean_max[x,1]))\n",
    "    for ch2 in ch_uniq_ALL:\n",
    "        x2 = np.where(max_I_all[:,0] == ch2)[0]\n",
    "        means_from_all_maxes.append(np.mean(max_I_all[x2,1]))\n",
    "        std_from_all_maxes.append(np.std(max_I_all[x2,1]))\n",
    "    meanofmeanmax = [ch_uniq,means]    \n",
    "    stats_per_ch = [ch_uniq_ALL, means_from_all_maxes, std_from_all_maxes]\n",
    "    \n",
    "    return np.transpose(meanofmeanmax), np.transpose(stats_per_ch), popmean, popstd # ['channel','mean of mean max'] ['channel', 'mean of maxes for all channels', 'std of maxes for all channels']\n",
    "\n",
    "#test for significance\n",
    "from scipy import stats as st\n",
    "\n",
    "#perform 1-sample t test\n",
    "def stu_ttest_per_chn(split_1):\n",
    "    _ , max_I_all = ALL_mean_max(split_1)\n",
    "    _ , stats_ch_ALL, popmean, popstd = meanofmeanmax_per_ch(split_1)\n",
    "    max_I_all = np.transpose(max_I_all)\n",
    "    ch_uniq_ALL = np.unique(max_I_all[:,0])\n",
    "    stats_per_chn = []\n",
    "    for ch2 in ch_uniq_ALL:\n",
    "        x2 = np.where(max_I_all[:,0] == ch2)[0]\n",
    "        stats = st.ttest_1samp(a=max_I_all[x2,1], popmean=popmean)\n",
    "        stats_per_chn.append(stats)\n",
    "    stats_per_chn_labeled = [ch_uniq_ALL, stats_per_chn];\n",
    "    return np.transpose(stats_per_chn_labeled)\n",
    "\n",
    "\n",
    "x = stu_ttest_per_chn(split_1)\n",
    "\n",
    "np.save('/gdrive/public/USERS/aguilac/Projects/FC_toolbox/results/mat_output/t_test_output.npy',x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e0afb-debb-4cf7-9c54-d479433c2aa4",
   "metadata": {},
   "source": [
    "- cant open the .mat file because I can't use mat73 on BOREL. so opening it in pieces might be worth it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
